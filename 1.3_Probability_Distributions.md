# Random Numbers

## Introduction to Random Numbers

Random numbers are values that lack any discernible pattern or predictability, appearing as if they were generated by chance. They are used to introduce uncertainty and variability into simulations, experiments, and applications where true randomness is desired.

There are two primary types of random numbers:
1. **Pseudo-Random Numbers:**
    - Pseudo-random numbers are generated using algorithms or mathematical formulas. They are deterministic and will produce the same sequence given the same initial conditions (seed).
    - These numbers are widely used in computer programs and simulations for their efficiency and repeatability. Common algorithms include the [Linear Congruential Generator (LCG)](https://en.wikipedia.org/wiki/Linear_congruential_generator) and the [Mersenne Twister](https://en.wikipedia.org/wiki/Mersenne_Twister).
2. **True Random Numbers:**
    - True random numbers are generated from inherently unpredictable physical processes or phenomena, such as radioactive decay or atmospheric noise. They are genuinely random and not generated by any algorithm.
    - True random number generators (TRNGs) are used in applications requiring a high degree of unpredictability and security, such as cryptography.
    - Intel microprosessors have special hardware and a special machine instruction, [RDRAND](https://en.wikipedia.org/wiki/RDRAND), for generating true random numbers

Random numbers enable the introduction of uncertainty and variability into models.

## Properties of Random Numbers

Random numbers, whether generated pseudorandomly or truly randomly, need to have two key properties:
- **Uniform Distribution:** Ideally, random numbers are uniformly distributed, meaning that each possible value has an equal probability of being selected. This property ensures fairness and unbiased randomness.
- **Independence:** Each random number is independent of all previous and future numbers in the sequence. The occurrence of one number does not affect the likelihood of any other number appearing.

## Probability distributions

Probability distribution is a fundamental concept in statistics and probability theory that provides a framework for understanding and modeling uncertainty and randomness in various phenomena. It describes the likelihood of different outcomes or events occurring in a random experiment or process. In other words, it defines how the possible values of a random variable are spread or distributed.

Probability distributions can be categorized into two main types:
- **Discrete Probability Distribution**: In this type, the random variable can only assume distinct, separate values. Examples include the Bernoulli distribution, binomial distribution, and Poisson distribution.
- **Continuous Probability Distribution**: In this type, the random variable can take on any value within a range. Common continuous distributions include the normal distribution, exponential distribution, and uniform distribution.

Some common probability distributions include:
- **Normal Distribution**: Often called the bell curve, it is used to model many natural phenomena and is central to statistical inference.
- **Binomial Distribution**: It is generally applied to experiments in which the result is one of a small number of possible final states, such as the number of "heads" or "tails" in a series of coin tosses. Used to model the number of successes in a fixed number of independent [Bernoulli trials](https://en.wikipedia.org/wiki/Bernoulli_trial).
- **Exponential Distribution**: Models the time between events in a Poisson process, such as the time between customer arrivals at a service center.
- **Poisson Distribution**: Describes the number of events occurring in fixed intervals of time or space when events are rare and random.

### Example of binomial distribution, Coin Flips

Suppose you are flipping a fair coin, which has two possible outcomes: "Heads" (H) and "Tails" (T). You are interested in finding out how many times you will get "Heads" when you flip the coin 10 times.

In this scenario, each coin flip can be considered a Bernoulli trial with two possible outcomes: "Heads" (success) with a probability of 0.5 and "Tails" (failure) with a probability of 0.5.

The binomial distribution can be used to model the number of "Heads" (successes) you will get in 10 coin flips. Let's denote:
- $n$ = 10 (the number of trials or coin flips)
- $p$ = 0.5 (the probability of success on each trial, which is getting "Heads" in this case)

Now, we want to find the probability of getting a specific number of "Heads" ($k$) in 10 coin flips. The probability mass function (PMF) of the binomial distribution allows us to calculate this:
$$P(X=k)={n\choose k}\cdot p^kq^{n-k}$$
Where:
- $P(X=k)$ is the probability of getting exactly $k$ "heads" in 10 coin flips
- ${n\choose k}$ represents the binomial coefficient, which is the number of ways to choose $k$ successes out of $n$ trials. It's calculated as C(n, k) = n! / (k! * (n-k)!)
- $p^k$ is the probability of $k$ successes (getting "Heads" $k$ times).
- $(1-p)^(n-k)$ is the probability of $(n-k)$ failures (getting "Tails" $(n-k)$ times).

Let's calculate a specific probability from this example:
Probability of getting exactly 5 "Heads" ($k$ = 5) in 10 coin flips:
$$P(X = 5) = {10\choose 5} \cdot (0.5)^5 \cdot (0.5)^(10-5)$$
Using the binomial coefficient, ${10 choose 5}$ = 252.
$$P(X = 5) = 252 * (0.5)^5 * (0.5)^5 = 252 * 0.03125 = 7.875%$$

So, the probability of getting exactly 5 "Heads" in 10 coin flips is approximately 7.875%. This demonstrates the application of the binomial distribution in modeling the outcomes of a series of independent trials with two possible outcomes.

![](images/binomdist.png)

## Random Numbers in Java

<!-- http://akira.ruc.dk/~keld/research/JAVASIMULATION/JAVASIMULATION-1.1/packages/javaSimulation/Docs/javasimulation.random.html -->
<!-- http://akira.ruc.dk/~keld/teaching/DAT_C_e01/Opgavekode/javaSimulation/random/src/Random.java -->