# Random Numbers

## Introduction to Random Numbers

Random numbers are values that lack any discernible pattern or predictability, appearing as if they were generated by chance. They are used to introduce uncertainty and variability into simulations, experiments, and applications where true randomness is desired.

There are two primary types of random numbers:
1. **Pseudo-Random Numbers:**
    - Pseudo-random numbers are generated using algorithms or mathematical formulas. They are deterministic and will produce the same sequence given the same initial conditions (seed).
    - These numbers are widely used in computer programs and simulations for their efficiency and repeatability. Common algorithms include the [Linear Congruential Generator (LCG)](https://en.wikipedia.org/wiki/Linear_congruential_generator) and the [Mersenne Twister](https://en.wikipedia.org/wiki/Mersenne_Twister).
2. **True Random Numbers:**
    - True random numbers are generated from inherently unpredictable physical processes or phenomena, such as radioactive decay or atmospheric noise. They are genuinely random and not generated by any algorithm.
    - True random number generators (TRNGs) are used in applications requiring a high degree of unpredictability and security, such as cryptography.
    - Intel microprosessors have special hardware and a special machine instruction, [RDRAND](https://en.wikipedia.org/wiki/RDRAND), for generating true random numbers

Random numbers enable the introduction of uncertainty and variability into models.

## Properties of Random Numbers

Random numbers, whether generated pseudorandomly or truly randomly, need to have two key properties:
- **Uniform Distribution:** Ideally, random numbers are uniformly distributed, meaning that each possible value has an equal probability of being selected. This property ensures fairness and unbiased randomness.
- **Independence:** Each random number is independent of all previous and future numbers in the sequence. The occurrence of one number does not affect the likelihood of any other number appearing.

## Probability distributions

Probability distribution is a fundamental concept in statistics and probability theory that provides a framework for understanding and modeling uncertainty and randomness in various phenomena. It describes the likelihood of different outcomes or events occurring in a random experiment or process. In other words, it defines how the possible values of a random variable are spread or distributed.

Probability distributions can be categorized into two main types:
- **Discrete Probability Distribution**: In this type, the random variable can only assume distinct, separate values. Examples include the Bernoulli distribution, binomial distribution, and Poisson distribution.
- **Continuous Probability Distribution**: In this type, the random variable can take on any value within a range. Common continuous distributions include the normal distribution, exponential distribution, and uniform distribution.

Some common probability distributions include:
- **Normal Distribution**: Often called the bell curve, it is used to model many natural phenomena and is central to statistical inference.
- **Binomial Distribution**: It is generally applied to experiments in which the result is one of a small number of possible final states, such as the number of "heads" or "tails" in a series of coin tosses. Used to model the number of successes in a fixed number of independent [Bernoulli trials](https://en.wikipedia.org/wiki/Bernoulli_trial).
- **Exponential Distribution**: Models the time between events in a [Poisson process](https://builtin.com/data-science/poisson-process), such as the time between customer arrivals at a service center.
- **Poisson Distribution**: Describes the number of events occurring in fixed intervals of time or space when events are rare and random.

### Example of binomial distribution, Coin Flips

Suppose you are flipping a fair coin, which has two possible outcomes: "Heads" (H) and "Tails" (T). You are interested in finding out how many times you will get "Heads" when you flip the coin 10 times.

In this scenario, each coin flip can be considered a Bernoulli trial with two possible outcomes: "Heads" (success) with a probability of 0.5 and "Tails" (failure) with a probability of 0.5.

The binomial distribution can be used to model the number of "Heads" (successes) you will get in 10 coin flips. Let's denote:
- $n$ = 10 (the number of trials or coin flips)
- $p$ = 0.5 (the probability of success on each trial, which is getting "Heads" in this case)

Now, we want to find the probability of getting a specific number of "Heads" ($k$) in 10 coin flips. The probability mass function (PMF) of the binomial distribution allows us to calculate this:
$$P(X=k)={n\choose k}\cdot p^kq^{n-k}$$
Where:
- $P(X=k)$ is the probability of getting exactly $k$ "heads" in 10 coin flips
- ${n\choose k}$ represents the [binomial coefficient](https://en.wikipedia.org/wiki/Binomial_coefficient), which is the number of ways to choose $k$ successes out of $n$ trials. It's calculated as $C(n, k) = n! / (k! \cdot (n-k)!)$
- $p^k$ is the probability of $k$ successes (getting "Heads" $k$ times).
- $(1-p)^{(n-k)}$ is the probability of $(n-k)$ failures (getting "Tails" $(n-k)$ times).

Let's calculate a specific probability from this example:
Probability of getting exactly 5 "Heads" ($k$ = 5) in 10 coin flips:
$$P(X = 5) = {10\choose 5} \cdot 0.5^5 \cdot 0.5^{(10-5)}$$
Using the binomial coefficient, ${10\choose 5}$ = 252.
$$P(X = 5) = 252 \cdot 0.5^5 \cdot 0.5^5 = 252 \cdot 0.0009766 = 24.61%$$

So, the probability of getting exactly 5 "Heads" in 10 coin flips is approximately 24.61%. This demonstrates the application of the binomial distribution in modeling the outcomes of a series of independent trials with two possible outcomes. We can plot the probabilities against different $k$ values as follows:

![](images/binomdist.png)

From the picture, we can see how the probability of a successful trial ($k$ number of "Heads" in our coin tossing) vary with different number "Heads" $k$ required the experiment to be successful. This kind of plot is probability distribution plot, where it is easy to see how probabilities are distributed among different parameter values.

When you increase the number of experiments $n$, binomial distribution starts to resemble normal distribution, as we can see from the following picture:

![](images/binomdist2.png)

Normal distribution is drawn using red color on the picture.

### Example of exponential distribution, Time Between Arrival of Customers

Suppose you are managing a small coffee shop, and you are interested in modeling the time between the arrivals of customers at your shop. You want to understand how long, on average, you can expect to wait before the next customer arrives.

In this scenario, you can use the exponential distribution to model the time between customer arrivals. The exponential distribution is often used for modeling the waiting time until the next event in a Poisson process, such as customer arrivals.

Let's assume that, on average, a customer arrives at your coffee shop every 10 minutes. This average arrival rate can be represented by the parameter 位 (lambda) in the exponential distribution. In this case, 位 = 1/10 per minute, meaning one customer per 10 minutes.

The probability density function (PDF) of the exponential distribution is given by:
$$f(x;\lambda)=\lambda e^{-\lambda x}, \text{for} x\geq 0$$
Where:
- f(x; 位) is the probability density function at a specific value x.
- 位 is the rate parameter (in this case, 1/10 per minute).
- $e$ is the base of the natural logarithm, approximately 2.71828.

This probability density function can be represented in graphical format:
![](images/expdist.png)

Now, you want to find the probability that the next customer arrives within a certain amount of time. For example, you may want to calculate the probability that a customer arrives within 5 minutes.

Using the PDF, you can calculate this probability:
$$P(X\leq 5) = \int_{0}^{5}\lambda e^{-\lambda x}dx$$

Calculating this integral gives you the probability that a customer arrives within 5 minutes.

This example demonstrates the use of the exponential distribution to model and calculate probabilities related to the time between events in a process with a constant arrival rate. In this case, it helps you understand the expected waiting time for the next customer to arrive at your coffee shop.



## Random Numbers in Java

<!-- http://akira.ruc.dk/~keld/research/JAVASIMULATION/JAVASIMULATION-1.1/packages/javaSimulation/Docs/javasimulation.random.html -->
<!-- http://akira.ruc.dk/~keld/teaching/DAT_C_e01/Opgavekode/javaSimulation/random/src/Random.java -->